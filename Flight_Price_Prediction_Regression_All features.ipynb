{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b81b643",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5913de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "\n",
    "# Import and suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceef712",
   "metadata": {},
   "source": [
    "# Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f45bbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airline</th>\n",
       "      <th>Date_of_Journey</th>\n",
       "      <th>Source</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Route</th>\n",
       "      <th>Dep_Time</th>\n",
       "      <th>Arrival_Time</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Total_Stops</th>\n",
       "      <th>Additional_Info</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IndiGo</td>\n",
       "      <td>24/03/2019</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>BLR → DEL</td>\n",
       "      <td>22:20</td>\n",
       "      <td>01:10 22 Mar</td>\n",
       "      <td>2h 50m</td>\n",
       "      <td>non-stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>3897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Air India</td>\n",
       "      <td>1/05/2019</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>CCU → IXR → BBI → BLR</td>\n",
       "      <td>05:50</td>\n",
       "      <td>13:15</td>\n",
       "      <td>7h 25m</td>\n",
       "      <td>2 stops</td>\n",
       "      <td>No info</td>\n",
       "      <td>7662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jet Airways</td>\n",
       "      <td>9/06/2019</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Cochin</td>\n",
       "      <td>DEL → LKO → BOM → COK</td>\n",
       "      <td>09:25</td>\n",
       "      <td>04:25 10 Jun</td>\n",
       "      <td>19h</td>\n",
       "      <td>2 stops</td>\n",
       "      <td>No info</td>\n",
       "      <td>13882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IndiGo</td>\n",
       "      <td>12/05/2019</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>CCU → NAG → BLR</td>\n",
       "      <td>18:05</td>\n",
       "      <td>23:30</td>\n",
       "      <td>5h 25m</td>\n",
       "      <td>1 stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>6218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IndiGo</td>\n",
       "      <td>01/03/2019</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>BLR → NAG → DEL</td>\n",
       "      <td>16:50</td>\n",
       "      <td>21:35</td>\n",
       "      <td>4h 45m</td>\n",
       "      <td>1 stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>13302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10678</th>\n",
       "      <td>Air Asia</td>\n",
       "      <td>9/04/2019</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>CCU → BLR</td>\n",
       "      <td>19:55</td>\n",
       "      <td>22:25</td>\n",
       "      <td>2h 30m</td>\n",
       "      <td>non-stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>4107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10679</th>\n",
       "      <td>Air India</td>\n",
       "      <td>27/04/2019</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>CCU → BLR</td>\n",
       "      <td>20:45</td>\n",
       "      <td>23:20</td>\n",
       "      <td>2h 35m</td>\n",
       "      <td>non-stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>4145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10680</th>\n",
       "      <td>Jet Airways</td>\n",
       "      <td>27/04/2019</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>BLR → DEL</td>\n",
       "      <td>08:20</td>\n",
       "      <td>11:20</td>\n",
       "      <td>3h</td>\n",
       "      <td>non-stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>7229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10681</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>01/03/2019</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>BLR → DEL</td>\n",
       "      <td>11:30</td>\n",
       "      <td>14:10</td>\n",
       "      <td>2h 40m</td>\n",
       "      <td>non-stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>12648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10682</th>\n",
       "      <td>Air India</td>\n",
       "      <td>9/05/2019</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Cochin</td>\n",
       "      <td>DEL → GOI → BOM → COK</td>\n",
       "      <td>10:55</td>\n",
       "      <td>19:15</td>\n",
       "      <td>8h 20m</td>\n",
       "      <td>2 stops</td>\n",
       "      <td>No info</td>\n",
       "      <td>11753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10683 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Airline Date_of_Journey    Source Destination  \\\n",
       "0           IndiGo      24/03/2019  Banglore   New Delhi   \n",
       "1        Air India       1/05/2019   Kolkata    Banglore   \n",
       "2      Jet Airways       9/06/2019     Delhi      Cochin   \n",
       "3           IndiGo      12/05/2019   Kolkata    Banglore   \n",
       "4           IndiGo      01/03/2019  Banglore   New Delhi   \n",
       "...            ...             ...       ...         ...   \n",
       "10678     Air Asia       9/04/2019   Kolkata    Banglore   \n",
       "10679    Air India      27/04/2019   Kolkata    Banglore   \n",
       "10680  Jet Airways      27/04/2019  Banglore       Delhi   \n",
       "10681      Vistara      01/03/2019  Banglore   New Delhi   \n",
       "10682    Air India       9/05/2019     Delhi      Cochin   \n",
       "\n",
       "                       Route Dep_Time  Arrival_Time Duration Total_Stops  \\\n",
       "0                  BLR → DEL    22:20  01:10 22 Mar   2h 50m    non-stop   \n",
       "1      CCU → IXR → BBI → BLR    05:50         13:15   7h 25m     2 stops   \n",
       "2      DEL → LKO → BOM → COK    09:25  04:25 10 Jun      19h     2 stops   \n",
       "3            CCU → NAG → BLR    18:05         23:30   5h 25m      1 stop   \n",
       "4            BLR → NAG → DEL    16:50         21:35   4h 45m      1 stop   \n",
       "...                      ...      ...           ...      ...         ...   \n",
       "10678              CCU → BLR    19:55         22:25   2h 30m    non-stop   \n",
       "10679              CCU → BLR    20:45         23:20   2h 35m    non-stop   \n",
       "10680              BLR → DEL    08:20         11:20       3h    non-stop   \n",
       "10681              BLR → DEL    11:30         14:10   2h 40m    non-stop   \n",
       "10682  DEL → GOI → BOM → COK    10:55         19:15   8h 20m     2 stops   \n",
       "\n",
       "      Additional_Info  Price  \n",
       "0             No info   3897  \n",
       "1             No info   7662  \n",
       "2             No info  13882  \n",
       "3             No info   6218  \n",
       "4             No info  13302  \n",
       "...               ...    ...  \n",
       "10678         No info   4107  \n",
       "10679         No info   4145  \n",
       "10680         No info   7229  \n",
       "10681         No info  12648  \n",
       "10682         No info  11753  \n",
       "\n",
       "[10683 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpp = pd.read_excel(\"Flight_Fare.xlsx\")\n",
    "fpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e794af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10683 entries, 0 to 10682\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Airline          10683 non-null  object\n",
      " 1   Date_of_Journey  10683 non-null  object\n",
      " 2   Source           10683 non-null  object\n",
      " 3   Destination      10683 non-null  object\n",
      " 4   Route            10682 non-null  object\n",
      " 5   Dep_Time         10683 non-null  object\n",
      " 6   Arrival_Time     10683 non-null  object\n",
      " 7   Duration         10683 non-null  object\n",
      " 8   Total_Stops      10682 non-null  object\n",
      " 9   Additional_Info  10683 non-null  object\n",
      " 10  Price            10683 non-null  int64 \n",
      "dtypes: int64(1), object(10)\n",
      "memory usage: 918.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#Basic checks: head, tail, datatypes, shape and describe\n",
    "fpp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83429436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10683, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43d5f633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10683.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9087.064121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4611.359167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1759.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5277.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8372.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12373.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>79512.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Price\n",
       "count  10683.000000\n",
       "mean    9087.064121\n",
       "std     4611.359167\n",
       "min     1759.000000\n",
       "25%     5277.000000\n",
       "50%     8372.000000\n",
       "75%    12373.000000\n",
       "max    79512.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "020d0861",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type '' not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#To see the Distribution of Categorical features\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fpp\u001b[38;5;241m.\u001b[39mdescribe(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:10940\u001b[0m, in \u001b[0;36mNDFrame.describe\u001b[1;34m(self, percentiles, include, exclude, datetime_is_numeric)\u001b[0m\n\u001b[0;32m  10691\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m  10692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdescribe\u001b[39m(\n\u001b[0;32m  10693\u001b[0m     \u001b[38;5;28mself\u001b[39m: NDFrameT,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10697\u001b[0m     datetime_is_numeric: bool_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m  10698\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m  10699\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m  10700\u001b[0m \u001b[38;5;124;03m    Generate descriptive statistics.\u001b[39;00m\n\u001b[0;32m  10701\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10938\u001b[0m \u001b[38;5;124;03m    max            NaN      3.0\u001b[39;00m\n\u001b[0;32m  10939\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m> 10940\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m describe_ndframe(\n\u001b[0;32m  10941\u001b[0m         obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10942\u001b[0m         include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[0;32m  10943\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m  10944\u001b[0m         datetime_is_numeric\u001b[38;5;241m=\u001b[39mdatetime_is_numeric,\n\u001b[0;32m  10945\u001b[0m         percentiles\u001b[38;5;241m=\u001b[39mpercentiles,\n\u001b[0;32m  10946\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\describe.py:101\u001b[0m, in \u001b[0;36mdescribe_ndframe\u001b[1;34m(obj, include, exclude, datetime_is_numeric, percentiles)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     describer \u001b[38;5;241m=\u001b[39m DataFrameDescriber(\n\u001b[0;32m     95\u001b[0m         obj\u001b[38;5;241m=\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, obj),\n\u001b[0;32m     96\u001b[0m         include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[0;32m     97\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     98\u001b[0m         datetime_is_numeric\u001b[38;5;241m=\u001b[39mdatetime_is_numeric,\n\u001b[0;32m     99\u001b[0m     )\n\u001b[1;32m--> 101\u001b[0m result \u001b[38;5;241m=\u001b[39m describer\u001b[38;5;241m.\u001b[39mdescribe(percentiles\u001b[38;5;241m=\u001b[39mpercentiles)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(NDFrameT, result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\describe.py:176\u001b[0m, in \u001b[0;36mDataFrameDescriber.describe\u001b[1;34m(self, percentiles)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdescribe\u001b[39m(\u001b[38;5;28mself\u001b[39m, percentiles: Sequence[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m--> 176\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_data()\n\u001b[0;32m    178\u001b[0m     ldesc: \u001b[38;5;28mlist\u001b[39m[Series] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, series \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\describe.py:208\u001b[0m, in \u001b[0;36mDataFrameDescriber._select_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    206\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mselect_dtypes(\n\u001b[0;32m    209\u001b[0m         include\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude,\n\u001b[0;32m    210\u001b[0m         exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4723\u001b[0m, in \u001b[0;36mDataFrame.select_dtypes\u001b[1;34m(self, include, exclude)\u001b[0m\n\u001b[0;32m   4720\u001b[0m             converted_dtypes\u001b[38;5;241m.\u001b[39mappend(infer_dtype_from_object(dtype))\n\u001b[0;32m   4721\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfrozenset\u001b[39m(converted_dtypes)\n\u001b[1;32m-> 4723\u001b[0m include \u001b[38;5;241m=\u001b[39m check_int_infer_dtype(include)\n\u001b[0;32m   4724\u001b[0m exclude \u001b[38;5;241m=\u001b[39m check_int_infer_dtype(exclude)\n\u001b[0;32m   4726\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dtypes \u001b[38;5;129;01min\u001b[39;00m (include, exclude):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4720\u001b[0m, in \u001b[0;36mDataFrame.select_dtypes.<locals>.check_int_infer_dtype\u001b[1;34m(dtypes)\u001b[0m\n\u001b[0;32m   4718\u001b[0m         converted_dtypes\u001b[38;5;241m.\u001b[39mextend([np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32])\n\u001b[0;32m   4719\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4720\u001b[0m         converted_dtypes\u001b[38;5;241m.\u001b[39mappend(infer_dtype_from_object(dtype))\n\u001b[0;32m   4721\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfrozenset\u001b[39m(converted_dtypes)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:1696\u001b[0m, in \u001b[0;36minfer_dtype_from_object\u001b[1;34m(dtype)\u001b[0m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m   1688\u001b[0m         \u001b[38;5;66;03m# Handles cases like get_dtype(int) i.e.,\u001b[39;00m\n\u001b[0;32m   1689\u001b[0m         \u001b[38;5;66;03m# Python objects that are valid dtypes\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1692\u001b[0m         \u001b[38;5;66;03m# TypeError handles the float16 type code of 'e'\u001b[39;00m\n\u001b[0;32m   1693\u001b[0m         \u001b[38;5;66;03m# further handle internal types\u001b[39;00m\n\u001b[0;32m   1694\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m-> 1696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m infer_dtype_from_object(np\u001b[38;5;241m.\u001b[39mdtype(dtype))\n",
      "\u001b[1;31mTypeError\u001b[0m: data type '' not understood"
     ]
    }
   ],
   "source": [
    "#To see the Distribution of Categorical features\n",
    "fpp.describe(include=['1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011c5ba3",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb89f6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "PydanticImportError",
     "evalue": "`BaseSettings` has been moved to the `pydantic-settings` package. See https://docs.pydantic.dev/2.1.1/migration/#basesettings-has-moved-to-pydantic-settings for more details.\n\nFor further information visit https://errors.pydantic.dev/2.1.1/u/import-error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPydanticImportError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas_profiling\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Main module of pandas-profiling.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m.. include:: ../../README.md\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontroller\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pandas_decorator\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofile_report\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas_profiling\\controller\\pandas_decorator.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"This file add the decorator on the DataFrame object.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofile_report\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprofile_report\u001b[39m(df: DataFrame, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ProfileReport:\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Profile a DataFrame.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m        A ProfileReport of the DataFrame.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas_profiling\\profile_report.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvisions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisionsTypeset\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, Settings\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpectations_report\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExpectationsReport\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malerts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlertType\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas_profiling\\config.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Enum\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, Optional\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, BaseSettings, Field\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_merge_dictionaries\u001b[39m(dict1: \u001b[38;5;28mdict\u001b[39m, dict2: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    Recursive merge dictionaries.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    :return: Merged dictionary\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\__init__.py:210\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m    208\u001b[0m dynamic_attr \u001b[38;5;241m=\u001b[39m _dynamic_imports\u001b[38;5;241m.\u001b[39mget(attr_name)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dynamic_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _getattr_migration(attr_name)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m import_module\n\u001b[0;32m    214\u001b[0m module \u001b[38;5;241m=\u001b[39m import_module(_dynamic_imports[attr_name], package\u001b[38;5;241m=\u001b[39m__package__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\_migration.py:289\u001b[0m, in \u001b[0;36mgetattr_migration.<locals>.wrapper\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m import_string(REDIRECT_TO_V1[import_path])\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m import_path \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic:BaseSettings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticImportError(\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`BaseSettings` has been moved to the `pydantic-settings` package. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    291\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://docs.pydantic.dev/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/migration/#basesettings-has-moved-to-pydantic-settings \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor more details.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    293\u001b[0m     )\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m import_path \u001b[38;5;129;01min\u001b[39;00m REMOVED_IN_V2:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticImportError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimport_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` has been removed in V2.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mPydanticImportError\u001b[0m: `BaseSettings` has been moved to the `pydantic-settings` package. See https://docs.pydantic.dev/2.1.1/migration/#basesettings-has-moved-to-pydantic-settings for more details.\n\nFor further information visit https://errors.pydantic.dev/2.1.1/u/import-error"
     ]
    }
   ],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ecc0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(fpp, title = 'Flight Price Model Profile', html = { 'style': {'full_width' : True}})\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef3a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.boxplot(fpp.Price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2e04f",
   "metadata": {},
   "source": [
    "Price is having outliers. So we should use IQR method for this feature to treating outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd72be52",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491b8de",
   "metadata": {},
   "source": [
    "# Treating Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1afb464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#treating outlier with censoring / capping\n",
    "#bringing extreme high outlier values in upper values\n",
    "#bringing extreme low outlier values in lower values\n",
    "def find_boundaries(variable):\n",
    "    \n",
    "    q1  = fpp[variable].quantile(0.25)\n",
    "    q3  = fpp[variable].quantile(0.75)\n",
    "    iqr = q3  - q1\n",
    "    lower_range  = q1  - 1.5 * iqr #used to find the minimum value\n",
    "    upper_range  = q3 +  1.5 * iqr #max value\n",
    "    return lower_range , upper_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faed642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining lower range and upper range\n",
    "lower_Price, upper_Price = find_boundaries('Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d266673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform capping\n",
    "#where function need to check condition\n",
    "#data.column = numpy.fn(original col > upper range of col, need to change the value as upper range, in that column)\n",
    "fpp.Price = np.where(fpp.Price > upper_Price, upper_Price, fpp.Price)\n",
    "fpp.Price = np.where(fpp.Price > lower_Price, lower_Price, fpp.Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1942885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After treating outliers verify the Price\n",
    "sb.boxplot(fpp.Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b47d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for Null values\n",
    "fpp.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ad498",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41140619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recheck null values\n",
    "fpp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c85564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for NaN values\n",
    "fpp.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc32139",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5935f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e67c78",
   "metadata": {},
   "source": [
    "# Handling Categorical Data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4877554a",
   "metadata": {},
   "source": [
    "1. Nominal data --> data are not in any order --> OneHotEncoder is used in this case\n",
    "2. Ordinal data --> data are in any order --> LabelEncoder is used in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4daab1",
   "metadata": {},
   "source": [
    "Airline, Source and Destination are Nominal data.So we can use OneHotEncoder to changing from categorical data into\n",
    "Numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c9977",
   "metadata": {},
   "source": [
    "# Airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f92a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp[\"Airline\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310bc4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp[\"Airline\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e88997",
   "metadata": {},
   "source": [
    "Drop_first = True removes the first column which is created for the first unique value of a column.\n",
    "If we do not use drop_first = True, then n dummy variables will be created, and these predictors(n dummy variables)\n",
    "are themselves correlated which is known as multicollinearity and it, in turn, leads to Dummy Variable Trap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Airline = fpp[[\"Airline\"]]\n",
    "\n",
    "Airline = pd.get_dummies(Airline, drop_first= True)\n",
    "\n",
    "Airline.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f788d54",
   "metadata": {},
   "source": [
    "# Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb1c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp[\"Source\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90860125",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp[\"Source\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ebc603",
   "metadata": {},
   "outputs": [],
   "source": [
    "Source = fpp[[\"Source\"]]\n",
    "\n",
    "Source = pd.get_dummies(Source, drop_first= True)\n",
    "\n",
    "Source.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8b046",
   "metadata": {},
   "source": [
    "# Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f5efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp[\"Destination\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2786db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp[\"Destination\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e4b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Destination = fpp[[\"Destination\"]]\n",
    "\n",
    "Destination = pd.get_dummies(Destination, drop_first= True)\n",
    "\n",
    "Destination.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04772ba9",
   "metadata": {},
   "source": [
    "# Concatinating all the Nominal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concating all the encoded dataframes\n",
    "## when 2 or 3 dataframes have different columns so we need to use axis = 1\n",
    "fpp_ohen =pd.concat([Airline,Source, Destination],axis=1)\n",
    "fpp_ohen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0a23f",
   "metadata": {},
   "source": [
    "# Total_Stops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782df0c",
   "metadata": {},
   "source": [
    "Total_Stops feature is ordinal type.So perform Label encoding for this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.Total_Stops.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1803cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.Total_Stops.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb2977",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp['Total_Stops'] = le.fit_transform(fpp['Total_Stops'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e8ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.Total_Stops.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.Total_Stops.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe472b2e",
   "metadata": {},
   "source": [
    "#  Concatination all the categorical features (Nominal + Ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab7e14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fpp_c = pd.concat([fpp_ohen, fpp.Total_Stops], axis = 1)\n",
    "fpp_c.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ca86c",
   "metadata": {},
   "source": [
    "# Route"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a21794",
   "metadata": {},
   "source": [
    "Route is categorical feature which is ordinal with special character. we need to perform split function and can perform label encoder to convert from categorical to numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Route = fpp[[\"Route\"]]\n",
    "Route.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66080bb1",
   "metadata": {},
   "source": [
    "Route and Total_Stops are similar each other. Here 5 stops are there from index 0 to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "Route['Rt_0'] = Route['Route'].str.split('→').str[0]\n",
    "Route['Rt_1'] = Route['Route'].str.split('→').str[1]\n",
    "Route['Rt_2'] = Route['Route'].str.split('→').str[2]\n",
    "Route['Rt_3'] = Route['Route'].str.split('→').str[3]\n",
    "Route['Rt_4'] = Route['Route'].str.split('→').str[4]\n",
    "Route.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c19b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Route.fillna('None', inplace = True)\n",
    "Route.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba8717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le=LabelEncoder()\n",
    "\n",
    "for i in range(0,5):\n",
    "    col = 'Rt_' + str(i)\n",
    "    Route[col] = le.fit_transform(Route[col])\n",
    "    \n",
    "Route.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421cdea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Route.drop('Route', axis = 1, inplace = True)\n",
    "Route.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfba99f",
   "metadata": {},
   "source": [
    "# Handling Time-Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dbb583",
   "metadata": {},
   "source": [
    "# Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_duration(duration):\n",
    "    if len(duration.split()) == 2:\n",
    "        hours = int(duration.split()[0][:-1])\n",
    "        minutes = int(duration.split()[1][:-1])\n",
    "        return hours * 60 + minutes\n",
    "    else:\n",
    "        return int(duration[:-1]) * 60\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67075759",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp['Duration'] = fpp['Duration'].apply(convert_duration)\n",
    "fpp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80651794",
   "metadata": {},
   "source": [
    "# Dep_Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662bd8d",
   "metadata": {},
   "source": [
    "First coverting object to datatime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp['Dep_Time'] = pd.to_datetime(fpp['Dep_Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a716550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb98dd4e",
   "metadata": {},
   "source": [
    "Then extracting the hour and minutes from Dep_Time and Arrival_Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp['Dep_Time_in_hours'] = fpp['Dep_Time'].dt.hour\n",
    "fpp['Dep_Time_in_min'] = fpp['Dep_Time'].dt.minute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bcded3",
   "metadata": {},
   "source": [
    "# Arrival_Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518aba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp['Arrival_Time'] = pd.to_datetime(fpp['Arrival_Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31516f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp['Arrival_Time_in_hours'] = fpp['Arrival_Time'].dt.hour\n",
    "fpp['Arrival_Time_in_min'] = fpp['Arrival_Time'].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739708d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62306ea3",
   "metadata": {},
   "source": [
    "Dep_Time, Arrival_Time features are not required now. so we will remove these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd394bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.drop(['Dep_Time','Arrival_Time'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5299d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af943903",
   "metadata": {},
   "source": [
    "# Date_of_Journey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e1df49",
   "metadata": {},
   "source": [
    "First converting data/month/year to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f3e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp['Date_of_Journey'] = pd.to_datetime(fpp['Date_of_Journey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68589dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp['Date_of_Journey'].dt.year.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8f8f4",
   "metadata": {},
   "source": [
    "2019 in year is common for all the values.so we can eliminate this year column.No need to split the year.we can do only for Days and month.Extracting Days and month from Date_of_Journey feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2278e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp['Day'] = fpp['Date_of_Journey'].dt.day\n",
    "fpp['Month'] = fpp['Date_of_Journey'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83804701",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.drop(['Date_of_Journey'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dffc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdcbe59",
   "metadata": {},
   "source": [
    "# Additional_Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d486665",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp['Additional_Info'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1887ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp['Additional_Info'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e188a",
   "metadata": {},
   "source": [
    "There is no valid information. So we can discard this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c8a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.drop('Additional_Info', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a408381",
   "metadata": {},
   "source": [
    "As we encoded categorical features which are Airline, Source, Destination, Route so that we can drop all these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d31c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.drop(['Airline','Source','Destination','Route'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef4329",
   "metadata": {},
   "source": [
    "# Concatinating all the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d2c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fpp_c = pd.concat([fpp_ohen, fpp.Total_Stops], axis = 1)\n",
    "fpp_fe = pd.concat([fpp_c, Route, fpp], axis = 1)\n",
    "fpp_fe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39c15b0",
   "metadata": {},
   "source": [
    "# Feature Selection for Independent and dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e190ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp_fe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81773d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fpp_fe.drop('Price', axis =1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae01a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fpp_fe[['Price']]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ff91d",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a684b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the train data and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23279712",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e49c68",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce274448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model creation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#initialize the model\n",
    "fpp_fe_lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a920fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model ---> make your model to learn\n",
    "#x_train, y_train\n",
    "#x_test ----> model_prediction(sales)\n",
    "\n",
    "fpp_fe_lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78ea219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "#we are passing 20% input data to test the trained model\n",
    "y_predict = fpp_fe_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65662877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to use evalution metrics to see the model performance\n",
    "#mse\n",
    "#mae\n",
    "#r squared\n",
    "#adjusted r squared\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = (mean_squared_error(y_test , y_predict))\n",
    "print(np.sqrt(mse)) #RMSE\n",
    "print(r2_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b7640",
   "metadata": {},
   "source": [
    "# 2. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model \n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff249af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gini, entropy, information gain --->important metrics for ASM\n",
    "fpp_fe_dt  = DecisionTreeClassifier(criterion='gini', max_depth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a08ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit data to the model \n",
    "fpp_fe_dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6244ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred  =  fpp_fe_dt.predict(X_test) #CHECK ON 20 PERCENT TEST DATA\n",
    "y_train_pred  = fpp_fe_dt.predict(X_train) #CHECK ON 80 PERCENT TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a917315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score , precision_score , recall_score , f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb1b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6538cfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add1df13",
   "metadata": {},
   "source": [
    "# 3. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d5601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28de920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp_fe_rf  =  RandomForestClassifier(n_estimators=10,criterion='entropy',max_depth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0987cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpp_fe_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9278e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred1  = fpp_fe_rf.predict(X_train)\n",
    "y_test_pred1  =  fpp_fe_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_train, y_train_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test , y_test_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac947a52",
   "metadata": {},
   "source": [
    "# 4. ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5540a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model creation\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "fpp_fe_ann = MLPClassifier( hidden_layer_sizes=(50,3),\n",
    "                       learning_rate_init=0.1,\n",
    "                       max_iter=100,\n",
    "                       random_state=150) ## model object creation max_iter=Stopping parameter\n",
    "fpp_fe_ann.fit(X_train,y_train) ## training the data\n",
    "y_predict_proba = fpp_fe_ann.predict_proba(X_test) ## predicting the pro\n",
    "## bability of class\n",
    "y_predict1 = fpp_fe_ann.predict(X_test)\n",
    "y_train_predict = fpp_fe_ann.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1559ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating the model created\n",
    "print(\"Train accuracy :\",accuracy_score(y_train,y_train_predict))\n",
    "print(\"Test accuracy :\",accuracy_score(y_test,y_predict1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7139f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_predict1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076ea98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
